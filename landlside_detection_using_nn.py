# -*- coding: utf-8 -*-
"""Landlside detection using NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sPcOgey9-7fpwBHaAo43ZBh2ZyARUGWZ
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import time
from collections import Counter

import torch.nn as nn
from imblearn.over_sampling import SMOTE
import torch.optim as optim

from sklearn.metrics import confusion_matrix, classification_report

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/file/d/1ouYvXR2RsDxljSJ1HVQknugm2iS7775w/view'

import pandas as pd

# to get the id part of the file
id = link.split("/")[-2]

downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('Global_Landslide_Final_medium.csv')

df = pd.read_csv('Global_Landslide_Final_medium.csv')
print(df)

print("Start")

df = pd.read_csv('Global_Landslide_Final_medium.csv')

df = df.drop(columns=['formatted_date','event_date','event_day','event_title','fatality_count','injury_count','population',
                      'event_id','event_description','location_description','country_name','latitude','longitude',
                      'location_accuracy','gazeteer_distance'])
df = df.dropna(subset=['landslide_trigger'])

df['landslide_size'] = df['landslide_size'].replace('very_large', 'large')
print('Landslide size:', df.landslide_size.unique())

df['event_time'] = df['event_time'].replace('unknown', 'NaN')


df['event_time'] = df['event_time'].astype(str)
df['landslide_category'] = df['landslide_category'].astype(str)
df['country_code'] = df['country_code'].astype(str)
df['gazeteer_closest_point'] = df['gazeteer_closest_point'].astype(str)


df = pd.get_dummies(df, prefix=['landslide_category','landslide_trigger','country_code'],
                    columns=['landslide_category','landslide_trigger','country_code'])

df = df[df.landslide_size != "catastrophic"]
df = df[df.landslide_size != "unknown"]

q=0
class2idx_landslide_size={}
for i in np.unique(df['landslide_size']):
    class2idx_landslide_size[i]=q
    q+=1
idx2class = {v: k for k, v in class2idx_landslide_size.items()}
df["landslide_size"] = df["landslide_size"].astype('category')
df["landslide_size"] = df["landslide_size"].cat.codes
df["event_time"] = df["event_time"].astype('category')
df["event_time"] = df["event_time"].cat.codes
df["gazeteer_closest_point"] = df["gazeteer_closest_point"].astype('category')
df["gazeteer_closest_point"] = df["gazeteer_closest_point"].cat.codes

Y = np.array(df['landslide_size'])


df = df.drop('landslide_size',axis=1)


# Save the column headers
feature_list = list(df.columns)
X = df.to_numpy() # Holds our data objects and attributes
X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.30)
print('Training Features Shape:', X_train.shape)
print('Training Labels Shape:', y_train.shape)
print('Testing Features Shape:', X_test.shape)
print('Testing Labels Shape:', y_test.shape)
# summarize class distribution
print("Before oversampling: ",Counter(y_train))

# define oversampling strategy
SMOTE = SMOTE()

# fit and apply the transform
X_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(X_train, y_train)

# summarize class distribution
print("After oversampling: ",Counter(y_train_SMOTE))

# Splitting dataset into training and test set
X_trainval, X_test, y_trainval, y_test = train_test_split(X_train_SMOTE, y_train_SMOTE, test_size=0.1,stratify=y_train_SMOTE, random_state=69)

# Split train into train-val
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1,stratify=y_trainval,random_state=21)

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)
X_train, y_train = np.array(X_train), np.array(y_train)
X_val, y_val = np.array(X_val), np.array(y_val)
X_test, y_test = np.array(X_test), np.array(y_test)

class ClassifierDataset(Dataset):

    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    def __len__ (self):
        return len(self.X_data)

def get_class_distribution(obj):
    count_dict = {
        "small": 0,
        "medium": 0,
        "large": 0,
    }

    for i in obj:
        if i == 0:
            count_dict['small'] += 1
        elif i == 1:
            count_dict['medium'] += 1
        elif i == 2:
            count_dict['large'] += 1
        else:
            print("Check classes.")

    return count_dict

train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())
val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())
test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())

target_list = []
for _, t in train_dataset:
    target_list.append(t)

target_list = torch.tensor(target_list)
target_list = target_list[torch.randperm(len(target_list))]

class_count = [i for i in get_class_distribution(y_train).values()]
class_weights = torch.tensor([1-(class_count[i]/(class_count[0]+class_count[1]+class_count[2])) for i in range(len(class_count))],dtype=float)
print(class_weights)
class_weights_all = class_weights[target_list]
weighted_sampler = WeightedRandomSampler(
    weights=class_weights_all,
    num_samples=len(class_weights_all),
    replacement=True
)

EPOCHS = 150
BATCH_SIZE = 16
LEARNING_RATE = 0.0005
NUM_FEATURES = int(X.shape[1])
NUM_CLASSES = np.unique(Y).size

train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler)

val_loader = DataLoader(dataset=val_dataset, batch_size=1)
test_loader = DataLoader(dataset=test_dataset, batch_size=1)

class MulticlassClassification(nn.Module):
    def __init__(self, num_feature, num_class):
        super(MulticlassClassification, self).__init__()

        self.layer_1 = nn.Linear(num_feature, 512)
        self.layer_2 = nn.Linear(512, 128)
        self.layer_3 = nn.Linear(128, 64)
        self.layer_4 = nn.Linear(64, 32)
        self.layer_out = nn.Linear(32, num_class)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.2)
        self.batchnorm1 = nn.BatchNorm1d(512)
        self.batchnorm2 = nn.BatchNorm1d(128)
        self.batchnorm3 = nn.BatchNorm1d(64)
        self.batchnorm4 = nn.BatchNorm1d(32)

    def forward(self, x):
        x = self.layer_1(x)
        x = self.batchnorm1(x)
        x = self.relu(x)

        x = self.layer_2(x)
        x = self.batchnorm2(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.layer_3(x)
        x = self.batchnorm3(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.layer_4(x)
        x = self.batchnorm4(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.layer_out(x)

        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)

model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
print(model)

def multi_acc(y_pred, y_test):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)

    correct_pred = (y_pred_tags == y_test).float()
    acc = correct_pred.sum() / len(correct_pred)

    acc = torch.round(acc * 100)

    return acc

accuracy_stats = {
    'train': [],
    "val": []
}
loss_stats = {
    'train': [],
    "val": []
}

print("Begin training.")


for e in range(1, EPOCHS+1):
    train_epoch_loss = 0
    train_epoch_acc = 0
    model.train()
    for X_train_batch, y_train_batch in train_loader:
        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)
        optimizer.zero_grad()

        y_train_pred = model(X_train_batch)

        train_loss = criterion(y_train_pred, y_train_batch)
        train_acc = multi_acc(y_train_pred, y_train_batch)

        train_loss.backward()
        optimizer.step()

        train_epoch_loss += train_loss.item()
        train_epoch_acc += train_acc.item()


    # VALIDATION
    with torch.no_grad():

        val_epoch_loss = 0
        val_epoch_acc = 0

        model.eval()
        for X_val_batch, y_val_batch in val_loader:
            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)

            y_val_pred = model(X_val_batch)

            val_loss = criterion(y_val_pred, y_val_batch)
            val_acc = multi_acc(y_val_pred, y_val_batch)

            val_epoch_loss += val_loss.item()
            val_epoch_acc += val_acc.item()
    loss_stats['train'].append(train_epoch_loss/len(train_loader))
    loss_stats['val'].append(val_epoch_loss/len(val_loader))
    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))
    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))


    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')

import datetime as dt
# Waits for everything to finish running
#torch.cuda.synchronize()
curr_time=dt.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
#print("Time elapsed to Train: ", int(start.elapsed_time(end))/60000)
torch.save(model.state_dict(), f'Landslide_model_{curr_time}.h5')
train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})
train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={"index":"epochs"})
# Plot the dataframes
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))
sns.lineplot(data=train_val_acc_df, x = "epochs", y="value", hue="variable",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')
sns.lineplot(data=train_val_loss_df, x = "epochs", y="value", hue="variable", ax=axes[1]).set_title('Train-Val Loss/Epoch')
plt.savefig(f'Valuation of Model_{curr_time}.png', dpi=300, bbox_inches='tight')
plt.show()

y_pred_list = []
with torch.no_grad():
    model.eval()
    for X_batch, _ in test_loader:
        X_batch = X_batch.to(device)
        y_test_pred = model(X_batch)
        _, y_pred_tags = torch.max(y_test_pred, dim = 1)
        y_pred_list.append(y_pred_tags.cpu().numpy())
y_pred_list = [a.squeeze().tolist() for a in y_pred_list]
confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred_list)).rename(columns=idx2class, index=idx2class)
sns.heatmap(confusion_matrix_df, annot=True)
#plt.savefig(f'Confusion Matrix_{curr_time}.png', dpi=300, bbox_inches='tight')
plt.show()
#report = open(f'Classfication_report_{curr_time}.txt','w')
#report.write(classification_report(y_test, y_pred_list))
#report.close()
print(classification_report(y_test, y_pred_list))

df.columns

for i,j in test_loader:
  print(i,j)
  break

X.shape
len(feature_list)